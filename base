\section{Metodologia}
A metodologia adotada nesta pesquisa foi estruturada em quatro etapas principais, coleta de dados, tratamento e preparação, testes com algoritmos de machine learning e análise dos resultados. O pipeline seguido está ilustrado na Figura 1.

Inicialmente, partimos de um dataset referente ao trânsito da cidade de São Paulo. Para enriquecer a análise e estabelecer uma correlação entre o comportamento do trânsito e o crescimento populacional da capital, buscamos dados complementares no portal https://dadosabertos.sp.gov.br. De lá, extraímos um segundo dataset contendo informações sobre a população de São Paulo ao longo dos anos.

Na segunda etapa, realizamos a limpeza e padronização dos dados. Os datasets apresentavam ruídos, campos fora de padrão e diferentes codificações de texto (encodings), o que exigiu um processo cuidadoso de tratamento para garantir a compatibilidade e integridade das informações. Após a unificação dos dados, com as variáveis devidamente estruturadas, avançamos para a etapa de testes com algoritmos de aprendizado de máquina.

Os testes foram conduzidos com diferentes modelos, e os resultados foram armazenados para posterior comparação, tanto em termos de desempenho quanto de qualidade preditiva. Essa abordagem permitiu avaliar a eficácia dos algoritmos frente aos dados tratados e entender melhor a relação entre o crescimento populacional e o impacto no trânsito da cidade.

\subsection{Datasets}
O ponto de partida da pesquisa foi a seleção de um dataset sobre o trânsito da cidade de São Paulo. Este conjunto de dados se mostrou especialmente relevante por conter informações como dia, hora, região e, principalmente, o nível de congestionamento, variável central para os objetivos do estudo. Algumas colunas adicionais estavam presentes, mas foram descartadas por não contribuírem diretamente para a análise proposta.

Para estabelecer uma correlação entre o comportamento do trânsito e o crescimento populacional da cidade, foi incorporado um segundo dataset, obtido por meio do portal dadosabertos.gov.sp Este conjunto trazia dados populacionais da capital paulista ao longo dos anos, incluindo as variáveis ano, distrito, sexo, faixa etária e população total.

Apesar da riqueza informacional, esse segundo dataset apresentou desafios técnicos. Havia ruídos nos dados, campos fora de padrão e codificações de texto distintas (encodings), o que exigiu um esforço adicional para uniformização e tratamento adequado. Um ponto específico que merece destaque é a estrutura da variável idade, que estava agrupada em faixas etárias (por exemplo, "10 a 14 anos", "15 a 19 anos"). Essa segmentação dificultou a filtragem precisa de indivíduos com idade mínima para conduzir veículos (a partir dos 18 anos), sendo possível apenas considerar faixas a partir dos 20 anos, o que introduz um pequeno desvio na análise.

A aquisição dos dados, portanto, envolveu não apenas a seleção criteriosa dos conjuntos mais relevantes, mas também a antecipação dos desafios que seriam enfrentados nas etapas seguintes de tratamento e modelagem.

\subsection{limpeza e preparacao}

Para assegurar a consistência e a qualidade dos dados utilizados na modelagem preditiva, foram desenvolvidos dois pipelines automatizados de pré-processamento, um voltado ao tratamento dos dados populacionais e outro dedicado ao conjunto de dados de congestionamentos de tráfego. Ambos os processos tiveram como objetivo padronizar formatos, remover inconsistências e preparar os dados para a etapa de integração e posterior aplicação de algoritmos de aprendizado de máquina.

O conjunto populacional passou por um tratamento mais detalhado, executado pelo script python . Inicialmente, todos os campos textuais foram normalizados por meio da remoção de acentuação e da padronização de espaços, garantindo uniformidade entre distritos e categorias. Em seguida, foi aplicado um filtro para manter apenas as faixas etárias com idade igual ou superior a 20 anos, de por conta de limitações do dataset, mais diretamente relacionada aos padrões de deslocamento urbano. Na sequência, foi criada uma nova variável denominada região, obtida a partir do mapeamento de cada distrito para uma das cinco grandes áreas geográficas de São Paulo: norte, sul, leste, oeste e centro. Esse mapeamento foi implementado por meio de um dicionário abrangendo os 96 distritos oficiais do município, permitindo uma agregação coerente com as divisões espaciais utilizadas nos dados de tráfego. Por fim, distritos que não possuíam correspondência foram identificados e registrados em um arquivo auxiliar para verificação manual, garantindo a completude do mapeamento antes da exportação final dos dados, que foram salvos em formato CSV com codificação UTF-8 padronizada.

O conjunto de dados de tráfego, processado pelo script python, exigiu um tratamento voltado principalmente à validação e à consistência das informações temporais e geográficas. As etapas iniciais incluíram a normalização dos campos textuais referentes a vias, regiões e expressways, removendo acentuação e espaços redundantes. Em seguida, foi realizada a validação das variáveis de data e hora, assegurando que apenas registros com formato temporal correto fossem mantidos. O campo de tamanho do congestionamento também passou por verificação, sendo eliminados valores negativos, nulos ou não numéricos. As regiões foram padronizadas para um formato em letras minúsculas e comparadas a uma lista de referência contendo as mesmas categorias usadas no dataset populacional, garantindo a compatibilidade entre as bases. Registros duplicados foram identificados e removidos, e, ao final, foi gerado um relatório estatístico que resumiu a distribuição dos congestionamentos por região, além de medidas descritivas como média, mediana e desvio padrão do tamanho dos congestionamentos.

Ambos os conjuntos de dados resultantes foram padronizados quanto à codificação e ao delimitador, de forma a assegurar compatibilidade durante o processo de integração. Essa padronização foi fundamental para que os datasets pudessem ser combinados por meio das variáveis região e ano, possibilitando análises conjuntas sobre o impacto da densidade populacional no comportamento do tráfego e permitindo que as etapas seguintes de modelagem preditiva fossem conduzidas de maneira consistente e reprodutível.
